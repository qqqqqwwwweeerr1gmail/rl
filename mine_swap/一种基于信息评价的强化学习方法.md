
About
=====

一种基于信息评价的强化学习方法

摘要
----

当前的机器学习技术取得了很大成功主要归结于大量的独立同分布的训练数据。但是由于无法对因果关系进行有效的理解，表现出很差的环境迁移能力。人类在认识世界、改造世界的过程中，善于从环境中总结、抽象知识，这种抽象的知识能轻松适应环境的变化，大大提高了知识的应用范围。为了模仿人类这种抽象能力，本文提出一种有模型的强化学习方法，它通过对环境建立知识图谱模型，不断假设环境中各种信息间的关联关系，并在交互中观察各种假设带来的收益大小，从而不断优化模型，挖掘出环境的知识图谱。我将它在一个自制的格子世界中进行试验，实验结果表明，该模型建模结果可以正确指导智能体在游戏不同变化中做出正确决策，并且很容易将其建模结果迁移到类似的环境中去。

引文
-----------

因果关系学习一直是人工智能领域的里程碑式的挑战。人类可以通过直觉，在没有明确指导的情况下，很简单地推断出不同元素之间的因果关系。目前大多数机器学习都比较成功，归结于对适当收集的独立同分布（independent and identically distributed，i.i.d.）数据进行大量的模式识别。但现实情况中，由于无法考虑和控制训练数据中的所有因素，分布往往会发生变化。要让机器学习模型在i.i.d.领域之外也起作用，不仅需要学习变量之间的统计关联，还需要学习潜在的因果模型[1]。
本文认为：在监督学习中加入因果关系模型效果会有很大局限性。首先，加入的因果关系模型并不能对因果关系本身进行评价、优化。其次，监督学习的领域知识信息片面，只有领域知识内的那些少量的因果关系，很难从大量的关系中进一步比较、总结出新的、更高阶的关系。
使用强化学习可以有效解决这两个缺陷。第一，强化学习通过与环境不断交互获得实验结果，理论上来说，它的数据样本是无限多的。第二，强化学习可以结合特定的搜索策略，对策略认为重要的方向进行优先搜索，从而提高强化学习的样本效率（sample efficiency）。
第三，在本文的强化学习算法中，搜索策略会基于不断扩展优化的知识图谱模型不断优化，从而使样本效率也能在不断的学习过程中持续地提高。

背景
-------

马尔可夫决策过程（MDP）是当前强化学习的基础。强化学习算法将机器与环境的探索过程看作是状态-行为对的转移，求解目标是使一个行为函数或状态函数收敛。这种基于马尔科夫决策过程的强化学习过分强调客观的状态转移，忽视了决策的主观性。这使得基于它的强化学习仅仅局限于学习状态-行为对，对于为什么要在这个状态下采取这个行为，并没有深入研究。
本文提出一种方法，它基于智能体观测到的信息，提出信息间关联关系的假设，并在环境中验证假设的正确性。这种方法，使得强化学习不仅仅局限于求一个价值函数，它的学习目标是找到越多的环境的内在关联关系。

算法介绍
-----

while True:
1.基于已有信息，提出一些全新猜测
2.继续玩很多轮，针对猜测进行剪枝模拟
3.MCTS 统计猜测命中率，评估猜测的决策行为有效性
4. 命中率接近random的舍去，高与低留下，进一步评估
5.对低的取反，高的猜测保留，理解有效猜测的周围信息（向下观测，向同级，向上）
6.（基于已有信息）猜测继续提很多个猜测，（对其猜测周围进行遍历）,总结猜测的关联关系，（向下观测），向同级，向上提出猜测
是否接近 最优模型期望收益：
break

    举一个吃苹果获得收益的例子：
    向下观测，就是我们观擦 苹果能吃这个命题的时候，要观察 '青苹果能吃' 和 '红苹果能吃' 是 '苹果能吃' 的子命题， 发现它是成立的。
    向同级观测，就是，在发现苹果能吃时，苹果属于集合水果，水果集合中有苹果、橘子、香蕉，分别提出假设，橘子、香蕉能否吃。
    向上级观测，大量都成立时，向上猜测。基于已有经验提出 '水果是否能吃' 这个猜测。
    为展现这种知识图谱的良好的迁移适应性,我将它用一个扫雷游戏中进行模拟，并通过改变棋盘的大小，布雷规则，观察模型的适应性。
    
    
    
扫雷游戏
-----

这是一个我自制的扫雷游戏。它的游戏规则是游戏初始时，左上角的格子会显示一个数字，代表其上下左右雷数，每列有且仅有一个雷，扫完全部非雷格子游戏结束。如果踩雷会获得-1的奖励。棋盘大小可以自定义。
显然，玩家掌握的信息越多，越容易通过游戏。
信息可以分为几类，1.是盘面能看到的信息。2.是通过数字及数字组合可以推断出来的信息。3.是通过多轮模拟，发现每列有且仅有一个雷的隐藏游戏规则。
通过蒙特卡洛随机模拟的方法，可以计算出当前信息水平下的奖励期望。及每个信息带来的边际奖励期望。
一个反复被证明有用的信息，将它存储到模型中，变成智能体的学习结果。

运行数据
-------

在一台普通CPU中（AMD Ryzen 7 3700X 8-Core Processor），运行了3分钟，共模拟1,000,000次游戏，通过假设1跳的关联关系 例：(数字2左下 必有雷)，挖掘出关联关系71条(文件edr.json)。使游戏的失败概率从 约0.86172，下降到0.773。 初步验证了该算法的收敛性。

总结
-------

基于MDP的强化学习，学到的是状态价值、行为价值函数，随着环境的细微改动就要重新学习。 而基于信息评价的强化学习，学到的是信息间的内在关联关系，对环境变化有很强的适应性。 这使得知识的应用范围变得不局限于某一特定环境。知识间的广泛连接，带来了通向AGI的曙光。

引文
------------

1. Why machine learning struggles with causality
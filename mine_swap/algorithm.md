basic
----

get mine falier
not get mine and left block is mine num success

method
-----------

1.save trajectory 
2.focus trajectory some content
3.conjecture trajectory
4.MC simulate conjecture trajectory benefit increase rate 
5.add correct conjecture to knowledge
6.
7.
8.
9.


About
=====

一种基于信息评价的强化学习方法




背景
-------

马尔可夫决策过程（MDP）是当前强化学习的基础。强化学习算法将机器与环境的探索过程看作是状态-行为对的转移，求解目标是使一个行为函数或状态函数收敛。这种基于马尔科夫决策过程的强化学习过分强调客观的状态转移，忽视了决策的主观性。这使得基于它的强化学习仅仅局限于学习状态-行为对，对于为什么要在这个状态下采取这个行为，并没有深入研究。
本文提出一种方法，它基于智能体观测到的信息，提出信息间关联关系的假设，并在环境中验证假设的正确性。这种方法，使得强化学习不仅仅局限于求一个价值函数，它的学习目标是找到越多的环境的内在关联关系。

算法介绍
-----

while True:
1.基于已有信息，提出一些全新猜测
2.继续玩很多轮，针对猜测进行剪枝模拟
3.MCTS 统计猜测命中率，评估猜测的决策行为有效性
4. 命中率接近random的舍去，高与低留下，进一步评估
5.对低的取反，高的猜测保留，理解有效猜测的周围信息（向下观测，向同级，向上）
6.（基于已有信息）猜测继续提很多个猜测，（对其猜测周围进行遍历）,总结猜测的关联关系，（向下观测），向同级，向上提出猜测
是否接近 最优模型期望收益：
break

    举一个吃苹果获得收益的例子：
    向下观测，就是我们观擦 苹果能吃这个命题的时候，要观察 '青苹果能吃' 和 '红苹果能吃' 是 '苹果能吃' 的子命题， 发现它是成立的。
    向同级观测，就是，在发现苹果能吃时，苹果属于集合水果，水果集合中有苹果、橘子、香蕉，分别提出假设，橘子、香蕉能否吃。
    向上级观测，大量都成立时，向上猜测。基于已有经验提出 '水果是否能吃' 这个猜测。
    为展现这种知识图谱的良好的迁移适应性,我将它用一个扫雷游戏中进行模拟，并通过改变棋盘的大小，布雷规则，观察模型的适应性。
    
    
    
扫雷游戏
-----

这是一个我自制的扫雷游戏。它的游戏规则是游戏初始时，左上角的格子会显示一个数字，代表其上下左右雷数，每列有且仅有一个雷，扫完全部非雷格子游戏结束。如果踩雷会获得-1的奖励。棋盘大小可以自定义。
显然，玩家掌握的信息越多，越容易通过游戏。
信息可以分为几类，1.是盘面能看到的信息。2.是通过数字及数字组合可以推断出来的信息。3.是通过多轮模拟，发现每列有且仅有一个雷的隐藏游戏规则。
通过蒙特卡洛随机模拟的方法，可以计算出当前信息水平下的奖励期望。及每个信息带来的边际奖励期望。
一个反复被证明有用的信息，将它存储到模型中，变成智能体的学习结果。

运行数据
-------

在一台普通CPU中（AMD Ryzen 7 3700X 8-Core Processor），运行了3分钟，共模拟1,000,000次游戏，通过假设1跳的关联关系 例：(数字2左下 必有雷)，挖掘出关联关系71条(文件edr.json)。使游戏的失败概率从 约0.86172，下降到0.773。 初步验证了该算法的收敛性。

总结
-------

基于MDP的强化学习，学到的是状态价值、行为价值函数，随着环境的细微改动就要重新学习。 而基于信息评价的强化学习，学到的是信息间的内在关联关系，对环境变化有很强的适应性。 这使得知识的应用范围变得不局限于某一特定环境。知识间的广泛连接，带来了通向AGI的曙光。

引文
------------

1. Why machine learning struggles with causality